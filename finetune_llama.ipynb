{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4de851e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d80207",
   "metadata": {},
   "source": [
    "## Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82692a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.7: Fast Llama patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 4. Max memory: 23.559 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.6. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "# 本质上还是对transformers的包装，但是做了一些优化\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "# 打开4bit量化加载\n",
    "# 所谓量化，是让模型以更低的空间存储这些参数，但是实际计算的时候要反量化尽量用原本的数去计算\n",
    "\n",
    "### Changing the model here is forbidden !\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-2-7b-bnb-4bit\",    ### Do not change the model for any other models or quantization versions\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94a0d8",
   "metadata": {},
   "source": [
    "## Add LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f31d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# TODO : Tweak the LoRA adapter hyperparameters here.  #####################\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.\n",
    "    lora_alpha = 16,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128\n",
    "\n",
    "# 这里在用unsloth的方法给大模型挂上LoRA适配器\n",
    "\n",
    "\n",
    "################# TODO  ####################################################################\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c91d5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",  ### Use llama-3.1 template for better performance here\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# 这里直接套用了模板，全套都是hugging face的东西\n",
    "# 改之前的样子：\n",
    "# messages = [\n",
    "#   {\"role\":\"user\", \"content\":\"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Write a short story in third person narration about a protagonist who has to make an important career decision.\"},\n",
    "#   {\"role\":\"assistant\", \"content\":\"John was at a crossroads in his life. He had just graduated college ...\"}\n",
    "# ]\n",
    "# 改之后的样子：\n",
    "# <|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "# Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Write a short story in third person narration about a protagonist who has to make an important career decision.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "# John was at a crossroads in his life. He had just graduated college and was now facing the big decision of what career to pursue. After much deliberation, he decided that he wanted to be an accountant and help the financially disadvantaged. He had always been good with numbers and enjoyed seeing the tangible results of his work. \n",
    "# John enrolled in accounting courses and initially found it quite challenging. He had to learn multiple systems and regulations quickly, but he worked hard and eventually excelled in his studies. After a few years,<|eot_id|>\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3fbf5",
   "metadata": {},
   "source": [
    "# Dataset Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c99b765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'conversations', 'score'],\n",
      "    num_rows: 52002\n",
      "})\n",
      "[{'content': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Give three tips for staying healthy.', 'role': 'user'}, {'content': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_from_disk(\"./fastchat_alpaca_52k\")\n",
    "print(dataset)\n",
    "print(dataset[0][\"conversations\"])\n",
    "# {\n",
    "#     \"id\" : ...,\n",
    "#     \"conversations\": [\n",
    "#         {\n",
    "#             'content': '...',\n",
    "#             'role':'user'\n",
    "#         },\n",
    "#         {\n",
    "#             'content': '...',\n",
    "#             'role':'assistent'\n",
    "#         }\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90defff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'conversations', 'score', 'text'],\n",
      "    num_rows: 52002\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Add a \"text\" field to each example\n",
    "# ---------------------------\n",
    "# This function extracts the first assistant message from the conversation\n",
    "def add_text_field(example):\n",
    "    # Extract the first message where role == 'assistant'\n",
    "    assistant_texts = [msg[\"content\"] for msg in example[\"conversations\"] if msg[\"role\"] == \"assistant\"] # example['conversation']是两个字典的列表\n",
    "    text = assistant_texts[0] if assistant_texts else \"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Map the function over the dataset to add the \"text\" column.\n",
    "dataset = dataset.map(add_text_field) # 这里的map是新增列\n",
    "\n",
    "# Print the dataset structure to confirm the new feature.\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a7db047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"identity_0\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"content\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Give three tips for staying healthy.\",\n",
      "      \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\",\n",
      "      \"role\": \"assistant\"\n",
      "    }\n",
      "  ],\n",
      "  \"score\": 4.5,\n",
      "  \"text\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(dataset[0],indent = 2,ensure_ascii = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3fe25ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top examples sorted by simple conversation length:\n",
      "ID: identity_45289, Conversation Length: 759\n",
      "ID: identity_6285, Conversation Length: 670\n",
      "ID: identity_15102, Conversation Length: 622\n",
      "ID: identity_18853, Conversation Length: 567\n",
      "ID: identity_15908, Conversation Length: 558\n",
      "\n",
      "Top examples sorted by advanced key (combination of conversation length and score):\n",
      "ID: identity_45289, Advanced Key Value: 6.018\n",
      "ID: identity_6285, Advanced Key Value: 5.84\n",
      "ID: identity_18295, Advanced Key Value: 5.6080000000000005\n",
      "ID: identity_35239, Advanced Key Value: 5.566\n",
      "ID: identity_39031, Advanced Key Value: 5.566\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "#################### TODO : Define a helper function for computing conversation length ###############\n",
    "\n",
    "# The default \"conversation length\" here refers to the length of the input (human) and output (gpt), you can modify it at your will\n",
    "\n",
    "def compute_conversation_length(example):\n",
    "    # Compute total word count across all messages in the 'conversations' field\n",
    "    return sum(len(message[\"content\"].split()) for message in example[\"conversations\"]) # 这里是把问答的词汇量都加起来\n",
    "\n",
    "\n",
    "#################### TODO ############################################################################\n",
    "\n",
    "# ---------------------------\n",
    "# Simple Sorting Method  (Default)\n",
    "# ---------------------------\n",
    "# Sort the dataset from shortest to longest conversation (by word count)\n",
    "sorted_dataset_simple_list = sorted(dataset, key=compute_conversation_length, reverse=True)\n",
    "\n",
    "# Convert back to a Dataset object\n",
    "sorted_dataset_simple = Dataset.from_list(sorted_dataset_simple_list)\n",
    "\n",
    "print(\"\\nTop examples sorted by simple conversation length:\")\n",
    "for entry in sorted_dataset_simple.select(range(5)):\n",
    "    print(f\"ID: {entry['id']}, Conversation Length: {compute_conversation_length(entry)}\")\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "\n",
    "############## Advanced Sorting Method (TODO : Modify the sorting key ##################\n",
    "# ---------------------------\n",
    "# Default : Sorting based on Combining conversation length with the 'score' field using a weighted sum.\n",
    "# Here, we multiply the score by 10 and add it to the conversation length.\n",
    "def advanced_sort_key(example):\n",
    "    conversation_len = compute_conversation_length(example)\n",
    "    score = example[\"score\"]\n",
    "    return 2e-3 * conversation_len + score * 1\n",
    "\n",
    "####################################### TODO ###########################################\n",
    "\n",
    "sorted_dataset_advanced_list = sorted(dataset, key=advanced_sort_key, reverse=True)\n",
    "# Convert back to a Dataset object\n",
    "sorted_dataset_advanced = Dataset.from_list(sorted_dataset_advanced_list)\n",
    "\n",
    "print(\"\\nTop examples sorted by advanced key (combination of conversation length and score):\")\n",
    "for entry in sorted_dataset_advanced.select(range(5)):\n",
    "    print(f\"ID: {entry['id']}, Advanced Key Value: {advanced_sort_key(entry)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bfea322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Standardizing formats (num_proc=64): 100%|██████████| 100/100 [00:18<00:00,  5.38 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 5855.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "################# TODO : select the simple or advanced dataset for training ##############\n",
    "\n",
    "dataset_used = \"sorted_dataset_simple\" #sorted_dataset_advanced\n",
    "\n",
    "################# TODO ###################################################################\n",
    "\n",
    "if dataset_used == \"sorted_dataset_simple\":\n",
    "    train_dataset = sorted_dataset_simple.select(range(0,100))    ### You can also select from the middle, e.g. sorted_dataset_simple.select(range(50,150))\n",
    "else:\n",
    "    train_dataset = sorted_dataset_advanced.select(range(0,100))\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "train_dataset = standardize_sharegpt(train_dataset)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "282e2205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'conversations', 'score', 'text'],\n",
      "    num_rows: 100\n",
      "})\n",
      "{\n",
      "  \"id\": \"identity_51486\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"content\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Create a workable schedule with the given tasks. ### Input: Tasks:\\n- Finish math exam preparation\\n- Research for upcoming project\\n- Clean the room\\n- Take evening jog\",\n",
      "      \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"Monday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog\\n\\nTuesday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog\\n\\nWednesday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog\\n\\nThursday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog\\n\\nFriday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog \\n\\nSaturday and Sunday:\\n- Spend Saturday and Sunday doing leisure activities.\",\n",
      "      \"role\": \"assistant\"\n",
      "    }\n",
      "  ],\n",
      "  \"score\": 4.0,\n",
      "  \"text\": \"<s><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Create a workable schedule with the given tasks. ### Input: Tasks:\\n- Finish math exam preparation\\n- Research for upcoming project\\n- Clean the room\\n- Take evening jog<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nMonday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog\\n\\nTuesday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog\\n\\nWednesday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog\\n\\nThursday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog\\n\\nFriday:\\n- 8 am to 10 am: Finish math exam preparation\\n- 10 am to 12 pm: Research for upcoming project\\n- 12 pm to 2 pm: Lunch Break\\n- 2 pm to 4 pm: Clean the room\\n- 4 pm to 5 pm: Break\\n- 5 pm to 7 pm: Take evening jog \\n\\nSaturday and Sunday:\\n- Spend Saturday and Sunday doing leisure activities.<|eot_id|>\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(train_dataset)\n",
    "print(json.dumps(train_dataset[99], indent = 2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6020f35",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec952123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Identify the odd one out. ### Input: Twitter, Instagram, Telegram',\n",
       "  'role': 'user'},\n",
       " {'content': 'Telegram', 'role': 'assistant'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54edaa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Telegram'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8904df",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c0201a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=68): 100%|██████████| 100/100 [00:04<00:00, 23.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "################# TODO : Tweak the training hyperparameters here.  #####################\n",
    "\n",
    "\n",
    "training_config = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,   ### Do not modify the seed for reproducibility\n",
    "}\n",
    "\n",
    "\n",
    "################# TODO #################################################################\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = training_config[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps = training_config[\"gradient_accumulation_steps\"],\n",
    "        warmup_steps = training_config[\"warmup_steps\"],\n",
    "        num_train_epochs = training_config[\"num_train_epochs\"], # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = training_config[\"learning_rate\"],\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = training_config[\"optim\"],\n",
    "        weight_decay = training_config[\"weight_decay\"],\n",
    "        lr_scheduler_type = training_config[\"lr_scheduler_type\"],\n",
    "        seed = training_config[\"seed\"],\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "92bdcdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<s><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Write a 3-page article discussing the impact of COVID-19 on the global economy<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe coronavirus (COVID-19) pandemic has had an unprecedented impact on the global economy. The spread of the virus has caused drastic disruptions to labor markets, supply chains, and consumer spending, as well as investors\\u2019 confidence in the markets. In this article, I will discuss the impact of COVID-19 on the global economy and its potential long-term impacts. \\n\\nFirst, businesses have been hit hard by the pandemic. Many have had to close their doors due to lockdowns, and even those who remain open are struggling to cope with decreased demand and disrupted supply chains. The economic losses from COVID-19 are estimated to be anywhere from $1.2 trillion to $2.7 trillion. \\n\\nSecond, the labor market has also been severely impacted by the pandemic. Unemployment has skyrocketed, hitting a five-year high of more than 15%, and is predicted to continue rising in the coming months. Job losses have been especially severe for workers in the hospitality, retail, and travel industries.\\n\\nThird, consumer spending has declined significantly as people have been faced with reduced incomes and less access to credit. This has resulted in businesses having to increase their prices in order to stay afloat. \\n\\nFourth, investor confidence has been shaken by the pandemic. Stock markets around the world have seen massive losses, particularly during the initial weeks of the pandemic. Although markets have rebounded in recent months, they remain unpredictable and volatility is expected to remain high. \\n\\nFinally, it is important to consider the potential long-term impacts of the pandemic on the global economy. The International Labour Organisation (ILO) estimates that the pandemic could be responsible for an additional 40 million job losses, and the economic losses from COVID-19 could take several years to recover from. In addition, increased protectionism and rising tensions between the US and China could further complicate the global economic recovery. \\n\\nIn conclusion, COVID-19 has had an immense impact on the global economy that has been felt by businesses, workers, and investors. The full extent of the economic losses from this pandemic and the potential long-term impacts remain to be seen.<|eot_id|>\"\n"
     ]
    }
   ],
   "source": [
    "sample = trainer.train_dataset[55][\"text\"]\n",
    "print(json.dumps(sample, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e2ecb081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainer.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "87e07a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=64): 100%|██████████| 100/100 [00:01<00:00, 62.74 examples/s] \n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Unsloth: All labels in your dataset are -100. Training losses will be all 0.\nFor example, are you sure you used `train_on_responses_only` correctly?\nOr did you mask our tokens incorrectly? Maybe this is intended?\nMaybe you're using a Llama chat template on a non Llama model for example?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_templates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_on_responses_only\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trainer = \u001b[43mtrain_on_responses_only\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstruction_part\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<|start_header_id|>user<|end_header_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_part\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<|start_header_id|>assistant<|end_header_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/hw5/lib/python3.12/site-packages/unsloth_zoo/dataset_utils.py:371\u001b[39m, in \u001b[36mtrain_on_responses_only\u001b[39m\u001b[34m(trainer, instruction_part, response_part, force_match, tokenizer, return_function, num_proc)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Check if all labels randomnly got masked to nothing - maybe wrong chat template?\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fix_zero_training_loss\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[43mfix_zero_training_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/hw5/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/hw5/lib/python3.12/site-packages/unsloth_zoo/training_utils.py:73\u001b[39m, in \u001b[36mfix_zero_training_loss\u001b[39m\u001b[34m(model, tokenizer, train_dataset)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m seen_bad == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m seen_good == \u001b[32m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m seen_bad / (seen_bad + seen_good) == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsloth: All labels in your dataset are -100. Training losses will be all 0.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m     75\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFor example, are you sure you used `train_on_responses_only` correctly?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOr did you mask our tokens incorrectly? Maybe this is intended?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMaybe you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using a Llama chat template on a non Llama model for example?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m     )\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m seen_bad / (seen_bad + seen_good) >= \u001b[32m0.9\u001b[39m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     81\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Nearly all labels in your dataset are -100. Training losses will be all 0.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m     82\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFor example, are you sure you used `train_on_responses_only` correctly?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m     83\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOr did you mask our tokens incorrectly? Maybe this is intended?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m     84\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMaybe you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using a Llama chat template on a non Llama model for example?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m     )\n",
      "\u001b[31mZeroDivisionError\u001b[39m: Unsloth: All labels in your dataset are -100. Training losses will be all 0.\nFor example, are you sure you used `train_on_responses_only` correctly?\nOr did you mask our tokens incorrectly? Maybe this is intended?\nMaybe you're using a Llama chat template on a non Llama model for example?"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3391f974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 2 | Total steps = 8\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 39,976,960 of 6,778,392,576 (0.59% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 01:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.549900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.477600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.450600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.578500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.385600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec7d3b",
   "metadata": {},
   "source": [
    "#### TODO : Curriculum Training  (Optional)\n",
    "start training the LLM with “easier” examples (e.g., shorter, clearer conversations) and progressively introduce more complex ones.\n",
    "\n",
    "The total data amount used to train should still not exceed 100 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb555a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TODO : Curriculum Training  ######################\n",
    "\n",
    "### E.g.\n",
    "### Step 1. Train on sorted_dataset_simple\n",
    "### Step 2. Train on sorted_dataset_advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177d362",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb117aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_true_output(text):\n",
    "    \"\"\"\n",
    "    Extracts the true assistant output from the decoded model output.\n",
    "\n",
    "    It looks for the assistant header token:\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    and extracts everything after it until the first occurrence of \"<|eot_id|>\".\n",
    "    If the assistant header is not found, it falls back to the last occurrence\n",
    "    of \"<|end_header_id|>\\n\\n\". If \"<|eot_id|>\" is not found, the extraction\n",
    "    continues until the end of the string.\n",
    "    \"\"\"\n",
    "    assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    start_index = text.find(assistant_header)\n",
    "    if start_index != -1:\n",
    "        start_index += len(assistant_header)\n",
    "    else:\n",
    "        # Fallback: use the last occurrence of the generic header ending\n",
    "        generic_header = \"<|end_header_id|>\\n\\n\"\n",
    "        start_index = text.rfind(generic_header)\n",
    "        if start_index != -1:\n",
    "            start_index += len(generic_header)\n",
    "        else:\n",
    "            start_index = 0\n",
    "\n",
    "    end_index = text.find(\"<|eot_id|>\", start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(text)\n",
    "    return text[start_index:end_index].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3d59f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/ML_Spring2025_HW5/test_set_evol_instruct_150.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m FastLanguageModel.for_inference(model) \u001b[38;5;66;03m# Enable native 2x faster inference\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Load the test set JSON file (without GPT responses)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/content/ML_Spring2025_HW5/test_set_evol_instruct_150.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[32m     13\u001b[39m     test_data = json.load(infile)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Dictionary to store inference results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/hw5/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/ML_Spring2025_HW5/test_set_evol_instruct_150.json'"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Load the test set JSON file (without GPT responses)\n",
    "with open(\"/content/ML_Spring2025_HW5/test_set_evol_instruct_150.json\", \"r\") as infile:\n",
    "    test_data = json.load(infile)\n",
    "\n",
    "# Dictionary to store inference results\n",
    "inference_results = {}\n",
    "\n",
    "# Loop over each data entry in the test set\n",
    "for index,entry in enumerate(test_data):\n",
    "    entry_id = entry.get(\"id\", \"unknown_id\")\n",
    "\n",
    "    # Build the messages list from the human conversation entries\n",
    "    # (Test set is expected to have only \"human\" messages)\n",
    "    messages = []\n",
    "    for conv in entry.get(\"conversations\", []):\n",
    "        if conv.get(\"from\") == \"human\":\n",
    "            messages.append({\"role\": \"user\", \"content\": conv.get(\"value\", \"\")})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": conv.get(\"value\", \"\")})\n",
    "\n",
    "    # Create inputs using the chat template (required for generation)\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Must add for generation\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "\n",
    "################# TODO : Tweak Decoding Parameters here.  #####################\n",
    "\n",
    "\n",
    "    # Generate model outputs\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=100,\n",
    "        use_cache=True,\n",
    "        temperature=1.5,\n",
    "        top_p = 0.9,\n",
    "        top_k = 30,\n",
    "    )\n",
    "\n",
    "\n",
    "################# TODO  ##########################################################\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    # Parse each output to extract the true assistant response\n",
    "    parsed_outputs = [parse_true_output(output) for output in decoded_outputs]\n",
    "\n",
    "    # Store the result for the current entry\n",
    "    inference_results[entry_id] = {\n",
    "        \"input\": messages,\n",
    "        \"output\": parsed_outputs\n",
    "    }\n",
    "\n",
    "    print(f\"Inference completed for entry {entry_id}\")\n",
    "\n",
    "\n",
    "#Write the inference results to the prediction JSON file\n",
    "with open(f\"pred.json\", \"w\") as outfile:\n",
    "    json.dump(inference_results, outfile, indent=4)\n",
    "with open(f\"training_config.json\", \"w\") as outfile:\n",
    "    json.dump(training_config, outfile, indent=4)\n",
    "\n",
    "from google.colab import files\n",
    "files.download('/content/pred.json')\n",
    "\n",
    "print(\"Inference completed for all entries in the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c82f49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
